{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5789bc3-b1ae-42c7-94a8-2ef4f89946fc",
   "metadata": {},
   "source": [
    "# Lesson 4: Persistence and Streaming for long running tasks\n",
    "\n",
    "### Persistence in LangChain:\n",
    "- **Definition**: Persistence allows an agent to save its state at a particular point in time. This stored state can then be reloaded and used in future interactions, enabling the agent to resume from where it left off. \n",
    "- **Use Case**: Persistence is crucial for long-running applications where an agent may need to maintain continuity over extended periods. For example, an agent helping with a multi-step task, such as project planning or writing, can store progress and resume when needed without losing context.\n",
    "\n",
    "### Streaming in LangChain:\n",
    "- **Definition**: Streaming allows an agent to emit signals in real-time about its current activities or progress. This capability provides insights into what the agent is doing at each step, allowing users or systems to monitor its operations in real-time.\n",
    "- **Use Case**: In long-running applications, streaming enables better interactivity by providing users with immediate feedback on the agent's status. For instance, in a live conversation or long-running process, streaming lets the user see updates as they happen, rather than waiting for the entire process to complete before receiving feedback.\n",
    "\n",
    "### Summary:\n",
    "- **Persistence**: Saves the agent's state to allow continuity in long-term applications.\n",
    "- **Streaming**: Provides real-time updates about the agent's operations for better tracking and interactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da06a64f-a2d5-4a66-8090-9ada0930c684",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ad950",
   "metadata": {
    "height": 30
   },
   "source": [
    "### checkpoiner for persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c033522-d2fc-41ac-8e3c-5e35872bf88d",
   "metadata": {
    "height": 121
   },
   "outputs": [],
   "source": [
    "# A check pointer basically checkpoints the state after and between every node.\n",
    "# To add in persistence for this agent, what we'll do is we'll use a SqliteSaver (in memory database). \n",
    "# But you can easily connect this to an external database or we also have other check pointers that use Redis and Postgres and other more persistent databases like that.\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 574
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        \n",
    "        # checkpoiner\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb83331",
   "metadata": {
    "height": 30
   },
   "source": [
    "### Streaming\n",
    "\n",
    "- First, we might care about streaming the individual messages. So this would be the Al message that determines what action to take. And then the observation message that represents the result of taking that action.\n",
    "- The second thing we might care about streaming is tokens. So for each token of the LLM call we might want to stream the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886bb4b",
   "metadata": {
    "height": 30
   },
   "source": [
    "We're now going to add this concept of a thread config. So this will be used to keep track of different threads inside the persistent checkpointer. This will allow us to have multiple conversations going on at the same time. This is really needed for production applications where you generally have many users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "714d1205-f8fc-4912-b148-2a45da99219c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 84
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_dYeseNNLTZ8gx8HhLOfMxMWb', 'function': {'arguments': '{\"query\":\"current weather in San Francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 151, 'total_tokens': 173}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4f4b1228-3c31-414b-83ab-f58249a4af53-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_dYeseNNLTZ8gx8HhLOfMxMWb'}])]\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_dYeseNNLTZ8gx8HhLOfMxMWb'}\n",
      "Back to the model!\n",
      "[ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1725460780, \\'localtime\\': \\'2024-09-04 07:39\\'}, \\'current\\': {\\'last_updated_epoch\\': 1725460200, \\'last_updated\\': \\'2024-09-04 07:30\\', \\'temp_c\\': 15.3, \\'temp_f\\': 59.5, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Partly cloudy\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/116.png\\', \\'code\\': 1003}, \\'wind_mph\\': 3.8, \\'wind_kph\\': 6.1, \\'wind_degree\\': 20, \\'wind_dir\\': \\'NNE\\', \\'pressure_mb\\': 1014.0, \\'pressure_in\\': 29.95, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 93, \\'cloud\\': 25, \\'feelslike_c\\': 15.3, \\'feelslike_f\\': 59.5, \\'windchill_c\\': 16.9, \\'windchill_f\\': 62.3, \\'heatindex_c\\': 16.9, \\'heatindex_f\\': 62.3, \\'dewpoint_c\\': 12.0, \\'dewpoint_f\\': 53.5, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 5.0, \\'gust_mph\\': 8.3, \\'gust_kph\\': 13.3}}\"}, {\\'url\\': \\'https://www.wunderground.com/hourly/us/ca/san-francisco/date/2024-09-02\\', \\'content\\': \\'Current Weather for Popular Cities . San Francisco, CA 60 ¬∞ F Partly Cloudy; Manhattan, NY 68 ¬∞ F Sunny; Schiller Park, IL (60176) warning 59 ¬∞ F Fair; Boston, MA 70 ¬∞ F Mostly Cloudy; Houston ...\\'}]', name='tavily_search_results_json', tool_call_id='call_dYeseNNLTZ8gx8HhLOfMxMWb')]\n",
      "[AIMessage(content='The current weather in San Francisco is partly cloudy with a temperature of approximately 15.3¬∞C (59.5¬∞F). The wind is blowing from the north-northeast at about 3.8 mph (6.1 kph). Humidity is high at 93%, and visibility is good at 16 km (9 miles). The UV index is moderate at 5.', response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 683, 'total_tokens': 762}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-ab73dbe2-467b-4b94-901a-ad76d92d83a5-0')]\n"
     ]
    }
   ],
   "source": [
    "# We won't use invoke, but will use stream this time\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302fb81",
   "metadata": {
    "height": 30
   },
   "source": [
    "### Vairous examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de483be7",
   "metadata": {
    "height": 30
   },
   "source": [
    "This time we're going to say \"What about in LA?\" So this is continuing the same conversation that we had before.\n",
    "It's asking a follow-up question. We don't say anything explicitly about the weather, but based on it being a conversation, we would expect it to realize that we're asking about the weather here.\n",
    "\n",
    "because we used the same `thread_id` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_oBN465Sil6jTfnMOsDTUxwQu', 'function': {'arguments': '{\"query\":\"current weather in Los Angeles\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 774, 'total_tokens': 796}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a05a8a9f-2d8b-47e5-9230-153aba2d42b9-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in Los Angeles'}, 'id': 'call_oBN465Sil6jTfnMOsDTUxwQu'}])]}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in Los Angeles'}, 'id': 'call_oBN465Sil6jTfnMOsDTUxwQu'}\n",
      "Back to the model!\n",
      "{'messages': [ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'Los Angeles\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 34.05, \\'lon\\': -118.24, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1725461028, \\'localtime\\': \\'2024-09-04 07:43\\'}, \\'current\\': {\\'last_updated_epoch\\': 1725460200, \\'last_updated\\': \\'2024-09-04 07:30\\', \\'temp_c\\': 24.9, \\'temp_f\\': 76.9, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Overcast\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/122.png\\', \\'code\\': 1009}, \\'wind_mph\\': 2.2, \\'wind_kph\\': 3.6, \\'wind_degree\\': 10, \\'wind_dir\\': \\'N\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 52, \\'cloud\\': 100, \\'feelslike_c\\': 25.8, \\'feelslike_f\\': 78.5, \\'windchill_c\\': 24.9, \\'windchill_f\\': 76.9, \\'heatindex_c\\': 25.8, \\'heatindex_f\\': 78.5, \\'dewpoint_c\\': 13.7, \\'dewpoint_f\\': 56.6, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 6.0, \\'gust_mph\\': 6.7, \\'gust_kph\\': 10.8}}\"}, {\\'url\\': \\'https://world-weather.info/forecast/usa/los_angeles/09-april/\\', \\'content\\': \\'Detailed ‚ö° Weather Forecast for April 9 in Los Angeles, California, United States - üå°Ô∏è temperature, wind, atmospheric pressure, humidity and precipitations - World-Weather.info. Add the current city. Search. Weather; Archive; ... April 07 April 08 Select date: April 10 April 11. April 09, 2024 : Atmospheric conditions and ...\\'}]', name='tavily_search_results_json', tool_call_id='call_oBN465Sil6jTfnMOsDTUxwQu')]}\n",
      "{'messages': [AIMessage(content='The current weather in Los Angeles is overcast with a temperature of approximately 24.9¬∞C (76.9¬∞F). The wind is coming from the north at about 2.2 mph (3.6 kph). Humidity is moderate at 52%, and visibility is good at 16 km (9 miles). The UV index is high at 6.', response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 1317, 'total_tokens': 1393}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-f72a36dc-81f5-4d0d-82ab-f19342eb55f9-0')]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='Los Angeles is warmer than San Francisco. The current temperature in Los Angeles is approximately 24.9¬∞C (76.9¬∞F), while the temperature in San Francisco is around 15.3¬∞C (59.5¬∞F).', response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 1405, 'total_tokens': 1452}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-163483ab-78e9-4327-b036-4b6a4612cbfa-0')]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='I need more context to provide an accurate answer. Are you comparing two specific locations, times, objects, or something else? Could you please provide more details?', response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 149, 'total_tokens': 182}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-47b926af-dffd-4335-8a07-2468f68f2330-0')]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}} # The LLM will really confuse, because it can't find the context from thread_id 1\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace59a36-3941-459e-b9d1-ac5a4a1ed3ae",
   "metadata": {},
   "source": [
    "## Streaming tokens\n",
    "\n",
    "A-stream event is an asynchronous method, which means that we're going to need to use an async checkpointer.\n",
    "In order to do this. We can import async SaliteSaver and pass that to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f921055",
   "metadata": {
    "height": 30
   },
   "source": [
    "What we want to do, is we want to look for events that correspond to new tokens. These kind of events are called `on_chat_model_stream`. When we see these events happening, we want to get the content and print it out. And we'll print it out with this type delimiter. When we run this, we should see it streaming real time into the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee0fe1c7-77e2-499c-a2f9-1f739bb6ddf0",
   "metadata": {
    "height": 210
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_OASyT15D0jV3Y7h6Z6T0h8pn'}\n",
      "Back to the model!\n",
      "The| current| weather| in| San| Francisco| is| partly| cloudy| with| a| temperature| of| |15|.|2|¬∞C| (|59|.|4|¬∞F|).| The| wind| is| blowing| from| the| north|-n|ort|heast| at| |6|.|1| k|ph| (|3|.|8| mph|).| Hum|idity| is| at| |93|%,| and| visibility| is| |16| km| (|9| miles|).| The| UV| index| is| |5|.|"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329a437",
   "metadata": {
    "height": 30
   },
   "source": [
    "It returns a final answer and stream out those tokens one at a time.\n",
    "\n",
    "We can see that we've got this little funny pipe delimiter, but we could easily remove that in our production application if we wanted to. So that's it for persistence and streaming. Pretty simple to get started with, but really powerful for building production applications.\n",
    "\n",
    "You're going to want your agents to be able to have multiple conversations at the same time, and have a concept of memory so they can resume those conversations. And you're also going to want them to be able to stream both the final tokens, but also all of the messages that came before.\n",
    "\n",
    "Persistence is also really important for enabling human in the loop type interactions, and that's exactly what we're going to cover in the next lesson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
